<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>数据挖掘概念总结 | Website of ZhiYuan Wang</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link href="//cdn.bootcss.com/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">主页</a></li>
      
      <li><a href="/categories/">分类</a></li>
      
      <li><a href="/tags/">标签</a></li>
      
      <li><a href="/about/">关于我</a></li>
      
      <li><a href="/friendly_links/">链接</a></li>
      
      <li><a href="/index.xml">订阅</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">数据挖掘概念总结</span></h1>
<h2 class="author">ZhiYuan Wang</h2>
<h2 class="date">2018/11/12</h2>
</div>

<main>
<div class="section level1">
<h1>1. 数据挖掘相关概念</h1>
<p>数据挖掘是一种技术，将传统数据分析方法（基于统计学）与处理大量数据的复杂算法（基于机器学习）相结合。</p>
<p>数据挖掘是在大型数据存储库中，自动地发现有用信息的过程。</p>
<p>所需要的知识：三大类</p>
<ul>
<li>数学类知识：概率论，统计学，机器学习；</li>
<li>计算机类知识：操作系统、数据库、版本控制、交互式编程、脚本编程、面向对象编程、面向函数编程；</li>
<li>业务知识：行业知识、商业知识、沟通与交流能力、演讲能力；</li>
</ul>
<p>任务：</p>
<ul>
<li>描述性任务：概括潜在的模式（关联、趋势、聚类、异常）；</li>
<li>预测类任务：回归、分类；</li>
</ul>
<p>流程：CRISP-DM (cross-industry standard process for data mining)</p>
<ul>
<li>商业理解：了解项目的需求和标的物；</li>
<li>数据理解：描述性统计，探索性分析，检测数据质量；</li>
<li>数据准备：将数据整理成建模所需的形式，即格式化数据；</li>
<li>构建模型：与数据准备阶段一起，反复实验多次；</li>
<li>评估：评价模型是否达到了应用标准；</li>
<li>部署：数据可视化，挖掘成果汇报；</li>
</ul>
<p>数据预处理：</p>
<ul>
<li>聚集（aggregation）；时间粒度和空间粒度；</li>
<li>抽样（sampling）；有放回抽样：bootstrap；</li>
<li>维规约；PCA，SVD；</li>
<li>特征子集选择；随机森林对特征的重要度排序，或自动回归的策略，利用AIC/BIC准则；</li>
<li>特征创建；特征提取、映射到新空间（傅里叶变换、小波变换）、特征构造；</li>
<li>离散化和二元化；将多分类问题转换为多个二分类问题；</li>
<li>変量変换，简单函数变换和标准化（standardization）、规范化（normalization）；</li>
</ul>
<p>相似性和相异性度量：</p>
<ul>
<li>欧氏距离</li>
<li>闵可夫斯基距离</li>
<li>曼哈顿距离</li>
<li>余弦距离</li>
</ul>
<p>探索性数据分析：</p>
<ul>
<li>描述性统计</li>
<li>可视化</li>
</ul>
<p>OLAP：联机分析处理，A是Analysis</p>
<ul>
<li>钻取：是改变维的层次，变换分析的粒度。它包括向下钻取（Drill-down）和向上钻取（Drill-up）/上卷(Roll-up)。Drill-up是在某一维上将低层次的细节数据概括到高层次的汇总数据，或者减少维数；而Drill-down则相反，它从汇总数据深入到细节数据进行观察或增加新维。</li>
<li>切片和切块：是在一部分维上选定值后，关心度量数据在剩余维上的分布。如果剩余的维只有两个，则是切片；如果有三个或以上，则是切块。</li>
<li>旋转：是变换维的方向，即在表格中重新安排维的放置（例如行列互换）。</li>
</ul>
<p>OLTP：联机事务处理，T是Transcation</p>
<p>也称为面向交易的处理过程，其基本特征是前台接收的用户数据可以立即传送到计算中心进行处理，并在很短的时间内给出处理结果，是对用户操作快速响应的方式之一。</p>
<p>ETL：extract,transform,load，构建数据仓库的重要步骤</p>
<p>数据的流向是从源数据流到ETL工具，ETL工具是一个单独的数据处理引擎，一般会在单独的硬件服务器上，实现所有数据转化的工作，然后将数据加载到目标数据仓库中，如果要增加整个ETL过程的效率，则只能增强ETL工具服务器的配置，优化系统处理流程（一般可调的东西非常少）。</p>
</div>
<div class="section level1">
<h1>2. 统计学方法</h1>
</div>
<div class="section level1">
<h1>3. 机器学习方法</h1>
<div class="section level2">
<h2>3.1 模型评估与选择</h2>
<p>误差分为训练误差和泛化误差，理想是泛化误差最小。</p>
<p>除模型的性能外，还要考虑：</p>
<ul>
<li>时间开销</li>
<li>存储开销</li>
<li>可解释性</li>
</ul>
<p>评估方法：</p>
<ul>
<li>留出法（hold-out）：一部分做训练，一部分做测试；单次留出结果可能不可靠，一般要多次随机留出；</li>
<li>交叉验证（cross-validation）：10次10折交叉验证；留一法，缺点计算量大。</li>
<li>自助法（bootstrap）：有放回的采样，适用于样本量较小。自助样本中大概有原始数据的63.2%的样本。</li>
</ul>
<p>验证集和测试集的区别：</p>
<ul>
<li>验证集，validation set，用于模型选择和调参；</li>
<li>测试集，testing set，用于估计实际使用时的泛化能力。</li>
</ul>
<p>性能度量：</p>
<ul>
<li>对于分类问题：
<ul>
<li>查准率（precision），查全率（recall），P-R曲线平衡点，查准率=查全率；</li>
<li>ROC曲线（Receiver Operating Characteristic），AUC面积</li>
<li>非均等代价（unequal cost）；</li>
<li>将假设检验应用于模型比较，判断是否在统计意义上更优；</li>
</ul></li>
<li>对于回归问题：
<ul>
<li>MSE;</li>
<li>MAE；</li>
</ul></li>
</ul>
<p>泛化误差的拆解：</p>
<p><span class="math display">\[泛化误差=偏差+方差+噪声\]</span></p>
<ul>
<li>偏差：算法的期望预测和真实结果的偏离程度，刻画算法本身的拟合能力；</li>
<li>方差：训练集变动所导致的学习性能变化，刻画数据扰动的影响；</li>
<li>噪声：学习任务本身难度；</li>
</ul>
<p>偏差与方差的矛盾就是过拟合与欠拟合的矛盾。</p>
<p>机器学习目标：建立具有良好泛化能力的模型。</p>
</div>
<div class="section level2">
<h2>3.2 基于树的方法</h2>
<div class="section level3">
<h3>3.2.1 决策树</h3>
<p>几种经典的决策树算法：Hunt/ID3/C4.5/CART</p>
<p>选择最佳划分的度量：熵、基尼、分类误差</p>
<p><span class="math display">\[Entropy(t)=-\sum_{i=0}^{c-1}{p(i|t)log_2{p(i|t)}}\]</span></p>
<p>熵越低，纯度越高；</p>
<p>熵差就是信息增益。比较父节点和子女节点的不纯程度，熵差越大，效果越好。</p>
<p>决策树是非参数方法，决策边界是直线，限制了决策树的复杂建模能力。</p>
<p>处理模型过拟合：先剪枝，后剪枝。</p>
</div>
<div id="bagging" class="section level3">
<h3>3.2.2 Bagging与随机森林</h3>
<p>集成学习中，个体学习器应尽可能独立。</p>
<p>bagging就是用bootstrap方法，采样出T个含m个训练样本的采样集，基于每个采样集训练基学习器，再结合。Bagging使用简单投票法集合结果。剩下36.8%的样本用于包外估计。Bagging用于降低方差。</p>
<p>随机森林：</p>
<ul>
<li>在样本上，使用bagging的策略；</li>
<li>在属性选择上，加入随机选择。</li>
</ul>
<p>随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动，这使得最终集成的泛化性能通过个体学习器之间差异度的增加而进一步提升。</p>
</div>
</div>
<div id="boosting" class="section level2">
<h2>3.3 集成学习：Boosting</h2>
<p>将弱学习器提升为强学习器。侧重于降低偏差。</p>
<p>先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注。</p>
<ul>
<li>重赋权法：在训练过程中的每一轮，样本分布为每个训练样本重新赋予一个权重。</li>
<li>重采样法：根据样本分布对训练集重新采样。</li>
</ul>
</div>
<div class="section level2">
<h2>3.4 支持向量机</h2>
<p>寻找一个划分超平面，将不同类别的样本分开。找到具有“最大间隔”的划分超平面。</p>
<p>划分超平面可被法向量和位移项唯一确定。</p>
<p>凸二次规划问题，利用拉格朗日乘子法得到对偶问题，满足KT条件，使用SMO算法求解。</p>
<p>核函数：原始样本空间非线性可分，映射到，高维特征空间使样本可分。使两向量在高维特征空间的内积转变为其在原始空间的函数变换。</p>
<ul>
<li>线性核</li>
<li>多项式核</li>
<li>高斯核</li>
<li>拉普拉斯核</li>
</ul>
<p>线性学习器—-核化—-&gt;非线性学习器</p>
<p>软间隔：不要求所有样本满足约束。</p>
<p><span class="math display">\[损失函数=结构风险+经验风险\]</span></p>
<ul>
<li>结构风险：用于描述模型的某些性质；</li>
<li>经验风险：用于描述模型与训练数据的切合程度。</li>
</ul>
<p>正则化项表述了我们希望获得具有何种性质的模型（例如复杂度较小的模型）。正则化可以理解为“罚函数法”，对不希望得到的结果施以惩罚，从而使优化过程趋向于希望目标。从贝叶斯的角度，正则化提供了模型的先验概率。</p>
<ol style="list-style-type: decimal">
<li>这为引入领域知识和用户意图提供了途径；</li>
<li>有助于削减假设空间，降低了最小化训练误差的过拟合风险。</li>
</ol>
</div>
<div class="section level2">
<h2>3.5 贝叶斯分类器</h2>
<p>贝叶斯视角下的机器学习：利用概率和误判损失来选择最优类别标记。</p>
<p>最小化分类错误率的贝叶斯最优分类器：</p>
<p><span class="math display">\[h^*(x)=arg_{c\in y}maxP(c|x)\]</span></p>
<p>对每个样本x，选择能使得后验概率P(c|x)最大的类别标记。</p>
<p><span class="math display">\[P(c|x)=\frac{P(c)P(x|c)}{P(x)}\]</span></p>
<ul>
<li>P(c)是类先验概率，空间中各类样本所占比例，根据大数定律，用频率估计概率。</li>
<li>P(x|c)是似然，likelihood；涉及x所有属性的联合概率。</li>
<li>P(x)是证据因子，对所有类标记均相同。</li>
</ul>
<p>假设P(x|c)具有确定的形式，并且被参数向量<span class="math inline">\(\Theta_c\)</span>唯一确定，我们的任务就是利用训练集D估计参数<span class="math inline">\(\Theta_c\)</span>。</p>
<p>关于参数估计的方法：</p>
<ul>
<li>频率主义学派：参数虽然未知，但是是客观存在的固定值，因此可以通过优化似然函数来确定参数值；</li>
<li>贝叶斯学派：参数是个未观测到的随机变量，可以假设参数服从一个先验分布，然后基于观测数据来计算参数的后验分布。</li>
</ul>
<p>P(x|c)的估计主要难点是，它是所有属性的联合概率。</p>
<p>朴素贝叶斯分类器做了“属性条件独立性的假设”，则</p>
<p><span class="math display">\[P(c|x)=\frac{P(c)P(x|c)}{P(x)}=\frac{P(c)}{P(x)}\prod_{i=1}^dP(x_i|c)\]</span></p>
<p>其中d为属性书目</p>
<p>朴素贝叶斯分类器的表达式：</p>
<p><span class="math display">\[h_{nb}(x)=arg_{c\in y}maxP(c)\prod_{i=1}^dP(x_i|c)\]</span></p>
<p>其中<span class="math inline">\(P(x_i|c)\)</span>为属性条件概率。</p>
</div>
<div id="em" class="section level2">
<h2>3.6 EM算法</h2>
<p>期望最大化算法，用于估计参数隐变量</p>
<ul>
<li>若参数<span class="math inline">\(\Theta\)</span>已知，则可根据训练数据推出最优隐变量Z的值；E步，利用当前估计的参数值计算对数似然的期望值；</li>
<li>若Z已知，泽科方便对参数<span class="math inline">\(\Theta\)</span>作极大似然估计；M步，寻找能使似然期望最大化的参数值。</li>
</ul>
<p>一个最直观了解 EM 算法思路的是 K-Means 算法。在 K-Means聚类时，每个聚类簇的质心是隐含数据。我们会假设 K 个初始化质心，即 EM 算法的 E 步；然后计算得到每个样本最近的质心，并把样本聚类到最近的这个质心，即 EM 算法的 M 步。重复这个 E 步和 M 步，直到质心不再变化为止，这样就完成了 K-Means 聚类。</p>
</div>
<div class="section level2">
<h2>3.7 关联分析</h2>
<p>购买计算机同时购买杀毒软件</p>
<ul>
<li>支持度：有用性，所有事务的2%显示计算机和杀毒软件被同时购买；</li>
<li>置信度：准确性，60%的购买计算机的顾客也买了杀毒软件；</li>
<li>提升度：一个的出现提升另一个出现的程度。</li>
</ul>
<p>关联规则是有趣的：满足最小支持度阈值和最小置信度阈值。</p>
<p>关联规则的挖掘可以归结为挖掘频繁项集；</p>
<ol style="list-style-type: decimal">
<li>找出所有频繁项集；总性能由该步决定。</li>
<li>由频繁项集产生强关联规则。</li>
</ol>
<p>Apriori先验算法，使用频繁项集的先验知识。</p>
</div>
</div>

</main>

  <footer>
  <script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script async src="//yihui.name/js/center-img.js"></script>
  
  <hr/>
  &copy; ZhiYuan Wang 2018
  
  </footer>
  </body>
</html>

